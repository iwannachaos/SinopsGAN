{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "G:\\Anaconda3\\envs\\test\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "G:\\Anaconda3\\envs\\test\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "G:\\Anaconda3\\envs\\test\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "G:\\Anaconda3\\envs\\test\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "G:\\Anaconda3\\envs\\test\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "G:\\Anaconda3\\envs\\test\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import SimpleRNN\n",
    "from keras.models import Sequential\n",
    "from keras.utils.vis_utils import plot_model\n",
    "import numpy as np\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "#from tensorflow.keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN():\n",
    "    def __init__(self, filename):\n",
    "        self.line_len = 0\n",
    "        self.img_rows = 28\n",
    "        self.img_cols = 28\n",
    "        self.channels = 1\n",
    "        self.predict_text = []\n",
    "        #self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        self.img_shape = (self.line_len, 1)\n",
    "        self.latent_dim = 100\n",
    "\n",
    "        optimizer = Adam(0.0002, 0.5)\n",
    "        self.prepare_text_data(filename)\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss='binary_crossentropy',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        # Build the generator\n",
    "        \n",
    "        self.generator = self.build_generator()\n",
    "\n",
    "        # The generator takes noise as input and generates imgs\n",
    "        z = Input(shape=(self.latent_dim,))\n",
    "        img = self.generator(z)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # The discriminator takes generated images as input and determines validity\n",
    "        validity = self.discriminator(img)\n",
    "\n",
    "        # The combined model  (stacked generator and discriminator)\n",
    "        # Trains the generator to fool the discriminator\n",
    "        self.combined = Model(z, validity)\n",
    "        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "    \n",
    "    # Подготовка текстовых данных: удаление функциональных символов в тексте\n",
    "    def prepare_text_data(self, string):\n",
    "        fin = open(string, 'rb')\n",
    "        self.lines = []\n",
    "\n",
    "        for line in fin:\n",
    "            line = line.strip().lower()\n",
    "            line = line.decode(\"ascii\",\"ignore\")\n",
    "            if len(line)==0:\n",
    "                continue\n",
    "            self.lines.append(line)\n",
    "        fin.close()\n",
    "        text = \" \".join(self.lines)\n",
    "        specs = \"',]\\\"{?-+_;$*[&)~|=/#}%<:>`!.()1234567890\"\n",
    "       \n",
    "        for sc in specs:\n",
    "            text = text.replace(sc, \"\")\n",
    "        \n",
    "   # Подготовка текстовых данных: создание словаря слов уникальных слов\n",
    "        words = text.split(' ')\n",
    "    \n",
    "        keywords = set([w for w in words])\n",
    "        nb_words = len(keywords)\n",
    "        self.words2index = dict((w,i) for i, w in enumerate(keywords))\n",
    "        self.index2words = dict((i,w) for i, w in enumerate(keywords))\n",
    "   \n",
    "   # Подготовка текстовых данных: удаление функциональных символов в массивах строк\n",
    "        self.lineswords = []\n",
    "    \n",
    "        self.clearlines = []\n",
    "        for l in self.lines:\n",
    "            for sc in specs:\n",
    "                l = l.replace(sc, \"\")\n",
    "            self.clearlines.append(l)\n",
    "        \n",
    "        for l in self.clearlines:\n",
    "            ls = l.split(' ')\n",
    "            self.lineswords.append(ls)\n",
    "            \n",
    "        #print(self.lineswords)\n",
    "        #print(self.clearlines)\n",
    "        \n",
    "    # Подготовка текстовых данных: удаление функциональных символов в массивах строк    \n",
    "        self.lineswordsdigit = self.lineswords.copy()\n",
    "        self.linesDigit = self.clearlines.copy()\n",
    "    # Подготовка текстовых данных: создание числовых массивов-слов   \n",
    "#         print(self.clearlines[547])\n",
    "#         print(self.lines[547])\n",
    "        for l in range (len(self.clearlines)):\n",
    "            for w in range(len(self.lineswords[l])):\n",
    "#                 print((l, w))\n",
    "                #                 if(self.lineswords[l][w] == 'years,'):\n",
    "                #                     print('lineswords[l][w]');\n",
    "\n",
    "                wdigit = self.words2index[self.lineswords[l][w]]\n",
    "                self.lineswordsdigit[l][w] = wdigit\n",
    "        \n",
    "    # Подготовка текстовых данных: заполнение массивов чисел до максимальной длины строки\n",
    "        maxl = 0\n",
    "        EMPTY_LINE_SYMBOL = '-1' \n",
    "        for l in range(len(self.lineswordsdigit)):\n",
    "            if len(self.lineswordsdigit[l])>maxl:\n",
    "                maxl = len(self.lineswordsdigit[l])\n",
    "        \n",
    "        print(\"------------------------------------------------------------------------------------\" + str(maxl))\n",
    "        self.line_len = maxl       \n",
    "        self.img_shape = (self.line_len, 1)\n",
    "        for l in range(len(self.lineswordsdigit)):\n",
    "            while len(self.lineswordsdigit[l])<maxl:\n",
    "                self.lineswordsdigit[l].append(EMPTY_LINE_SYMBOL)\n",
    "                    \n",
    "        \n",
    "  # Создание блока-генератора\n",
    "\n",
    "    def build_generator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(128, input_dim=self.latent_dim))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(512))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(1024))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(np.prod(self.img_shape), activation='tanh'))\n",
    "        model.add(Reshape(self.img_shape))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        noise = Input(shape=(self.latent_dim,))\n",
    "        img = model(noise)\n",
    "\n",
    "        return Model(noise, img)\n",
    "\n",
    "    # Создание блока-дешифратора\n",
    "\n",
    "    def build_discriminator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Flatten(input_shape=self.img_shape))\n",
    "        model.add(Dense(512))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(256))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        model.summary()\n",
    "\n",
    "        img = Input(shape=self.img_shape)\n",
    "        validity = model(img)\n",
    "\n",
    "        return Model(img, validity)\n",
    "\n",
    "   \n",
    "    def train(self, epochs, batch_size=128, sample_interval=1):\n",
    "\n",
    "        # Load the dataset\n",
    "      \n",
    "        X_train = np.array(self.lineswordsdigit).astype('int32')\n",
    "\n",
    "        # Rescale -1 to 1\n",
    "        X_train = ((X_train + 1) / (len(self.words2index)+1)-0.5)*2\n",
    "        X_train = np.expand_dims(X_train, axis=2)\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = np.ones((batch_size, 1))\n",
    "        fake = np.zeros((batch_size, 1))\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "\n",
    "            # Select a random batch of images\n",
    "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "            imgs = X_train[idx]\n",
    "\n",
    "            noise = np.random.uniform(-1, 1, (batch_size, self.latent_dim))\n",
    "\n",
    "            # Generate a batch of new images\n",
    "            gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "            # Train the discriminator\n",
    "            d_loss_real = self.discriminator.train_on_batch(imgs, valid)\n",
    "            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "\n",
    "            noise = np.random.uniform(-1, 1, (batch_size, self.latent_dim))\n",
    "\n",
    "            # Train the generator (to have the discriminator label samples as valid)\n",
    "            g_loss = self.combined.train_on_batch(noise, valid)\n",
    "\n",
    "            # Plot the progress\n",
    "            print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
    "\n",
    "            # If at save interval => save generated image samples\n",
    "#             if epoch % sample_interval == 0:\n",
    "#                 self.sample_text(epoch)\n",
    "    \n",
    "\n",
    "\n",
    "    def sample_text(self):\n",
    "        l = 10\n",
    "        noise = np.random.uniform (-1, 1, (l, self.latent_dim))\n",
    "        gen_txt = self.generator.predict(noise)\n",
    "\n",
    "\n",
    "        gen_txt = (gen_txt+1)*(len(self.words2index)+1)*0.5-1\n",
    "        gen_txt=np.rint(gen_txt)\n",
    "        gen_txt=np.array(gen_txt).astype('int32')\n",
    "        \n",
    "        t_text = gen_txt.copy()\n",
    "\n",
    "      \n",
    "        for l1 in range(len(gen_txt)):\n",
    "            self.predict_text.append([])\n",
    "            self.predict_text[0].append([])\n",
    "            count = 0\n",
    "            o_level = 0\n",
    "            i_level = 0\n",
    "            \n",
    "            for w1 in range(len(gen_txt[1])):\n",
    "                if (gen_txt[l1][w1][0]==-1) or (gen_txt[l1][w1][0] >= len(self.index2words)):\n",
    "                    t_text = \"\"    \n",
    "                    count+=1            \n",
    "                else:\n",
    "                    t_text = self.index2words[gen_txt[l1][w1][0]]\n",
    "                    self.predict_text[o_level][i_level].append(t_text)\n",
    "                    count+=1\n",
    "                if count == l:\n",
    "                    i_level+=1\n",
    "                    count=0\n",
    "                    self.predict_text.append([])\n",
    "                    self.predict_text[o_level].append([])\n",
    "                if i_level ==l:\n",
    "                    o_level+=1  \n",
    "                    i_level=0\n",
    "            \n",
    "            \n",
    "        file = open(\"generics_texts/\"+filename+\"_generic_text.txt\", \"w\")\n",
    "        for l_2 in self.predict_text:\n",
    "            file.write(str(l_2) + '\\n')\n",
    "        file.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "def mass_text_data():\n",
    "    for root, dirs, files in os.walk(\"Texts by genres\"):\n",
    "        for f in files:\n",
    "            models.append(GAN(\"Texts by genres/\"+f))\n",
    "            models[len(models)-1].train(10)\n",
    "            models[len(models)-1].sample_text()\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------3176\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 3176)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               1626624   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 1,758,209\n",
      "Trainable params: 1,758,209\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 128)               12928     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 512)               66048     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 3176)              3255400   \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 3176, 1)           0         \n",
      "=================================================================\n",
      "Total params: 3,866,344\n",
      "Trainable params: 3,863,016\n",
      "Non-trainable params: 3,328\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda3\\envs\\test\\lib\\site-packages\\keras\\engine\\training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 1.340861, acc.: 44.92%] [G loss: 1.168566]\n",
      "1 [D loss: 0.240631, acc.: 93.75%] [G loss: 1.076888]\n",
      "2 [D loss: 0.270760, acc.: 91.02%] [G loss: 1.029288]\n",
      "3 [D loss: 0.306828, acc.: 86.72%] [G loss: 0.992646]\n",
      "4 [D loss: 0.317484, acc.: 83.59%] [G loss: 0.968873]\n",
      "5 [D loss: 0.293040, acc.: 86.72%] [G loss: 1.009239]\n",
      "6 [D loss: 0.281132, acc.: 89.84%] [G loss: 1.029619]\n",
      "7 [D loss: 0.273683, acc.: 88.67%] [G loss: 1.036252]\n",
      "8 [D loss: 0.261608, acc.: 92.58%] [G loss: 1.112981]\n",
      "9 [D loss: 0.259941, acc.: 92.58%] [G loss: 1.159182]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-4bedc3a422d8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmass_text_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-3-f9377e33e696>\u001b[0m in \u001b[0;36mmass_text_data\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m             \u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mGAN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Texts by genres/\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m             \u001b[0mmodels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m             \u001b[0mmodels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-40d645a42052>\u001b[0m in \u001b[0;36msample_text\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    232\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m                     \u001b[0mt_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex2words\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mgen_txt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ml1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mw1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 234\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_text\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mo_level\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi_level\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    235\u001b[0m                     \u001b[0mcount\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mcount\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "mass_text_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gan = GAN()\n",
    "# # gan.prepare_text_data(\"3Ksentcs.txt\")\n",
    "# gan.train(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noise = np.random.uniform(-1, 1, (10, gan.latent_dim))\n",
    "# gen_txt = gan.generator.predict(noise)\n",
    "\n",
    "# gen_txt = (gen_txt+1)*(len(gan.words2index)+1)*0.5-1\n",
    "# gen_txt=np.rint(gen_txt)\n",
    "# gen_txt=np.array(gen_txt).astype('int32')\n",
    "\n",
    "# print(np.min(gen_txt))\n",
    "# print(np.max(gen_txt))\n",
    "# print(len(gan.index2words))\n",
    "\n",
    "# for l1 in range(len(gen_txt)):\n",
    "#     for w1 in range(len(gen_txt[1])):        \n",
    "#         t_text = gan.index2words[gen_txt[l1][w1][0]]\n",
    "#         gan.predict_text = t_text             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train = np.array(gan.lineswordsdigit).astype('int32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Rescale 0 to 1\n",
    "# X_train = (X_train + 1) / (len(gan.words2index)+1)\n",
    "# X_train = np.expand_dims(X_train, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gan.img_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
